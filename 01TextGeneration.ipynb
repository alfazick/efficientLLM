{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# !pip install -r requirements.txt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1 : Load the Model\n",
    "\n",
    "model_name = \"./models/gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2. How to generate a token from the model output tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  464,  1913, 10373, 11949,   460,   466]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Tokenize input prompt\n",
    "\n",
    "prompt = \"The strong ML engineer can do\"\n",
    "inputs = tokenizer(prompt, return_tensors =\"pt\")\n",
    "inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 50257])\n"
     ]
    }
   ],
   "source": [
    "# pass the inputs to the model and retrieve the logits \n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "logits = outputs.logits \n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(340)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_logits = logits[0,-1,:]\n",
    "next_token_id = last_logits.argmax()\n",
    "next_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' it'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decode the most likely token \n",
    "tokenizer.decode(next_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' it',\n",
       " ' a',\n",
       " ' this',\n",
       " ' the',\n",
       " ' anything',\n",
       " ' everything',\n",
       " ' all',\n",
       " ' things',\n",
       " ' what',\n",
       " ' much']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the 10 next words \n",
    "top_k = torch.topk(last_logits, k = 10)\n",
    "tokens = [tokenizer.decode(tk) for tk in top_k.indices]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_inputs = {\n",
    "    \"input_ids\":torch.cat(\n",
    "        [inputs[\"input_ids\"], next_token_id.reshape((1,1))],\n",
    "        dim=1\n",
    "    ),\n",
    "    \"attention_mask\": torch.cat(\n",
    "        [inputs[\"attention_mask\"], torch.tensor([[1]])],\n",
    "        dim = 1\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  464,  1913, 10373, 11949,   460,   466,   340]]) torch.Size([1, 7])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1]]) torch.Size([1, 7])\n"
     ]
    }
   ],
   "source": [
    "print(next_inputs[\"input_ids\"],\n",
    "      next_inputs[\"input_ids\"].shape)\n",
    "print(next_inputs[\"attention_mask\"],\n",
    "      next_inputs[\"attention_mask\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok so the idea is that model generates next word, by knowing what it said last time\n",
    "\n",
    "The input_ids and attention_mask are updated iteratively to reflect the growing sequence, ensuring that the model's predictions are based on all the previous context. This way, the model \"remembers\" what it has generated so far, which influences its predictions for the next words or tokens in the sequence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolutely, the generation process can't and shouldn't continue indefinitely. There are several strategies used to determine when a model should stop generating further tokens:\n",
    "\n",
    "Maximum Length: One common approach is to set a maximum sequence length. Once the generated sequence reaches this length, the generation process stops. This prevents excessively long outputs and ensures computational efficiency.\n",
    "End-of-Sequence Token: Many models are trained to generate a special token (often denoted as <EOS> for End Of Sequence) that signals the completion of a sentence or a logical end to the output. When the model generates this token, the process stops.\n",
    "Probability Threshold: In some cases, generation can be stopped if the model's confidence in its next predictions falls below a certain threshold. If the predicted probability of the next token is very low, it might indicate that the model is unsure how to continue meaningfully.\n",
    "Conditional Ending: For specific tasks, the generation might stop based on contextual conditions or rules. For example, in a question-answering task, the model might stop once it has generated what it determines to be a complete answer.\n",
    "Human Intervention: In interactive applications, a human might decide when the output is sufficient and manually stop the generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
