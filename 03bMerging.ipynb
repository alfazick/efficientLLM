{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# PyTorch Tutorial: Merging Variable Length Batches\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Example data\n",
    "\n",
    "batch1 = {\n",
    "    'input_ids': torch.tensor([[1]]),       # Single token\n",
    "    'attention_mask': torch.tensor([[1]])  # Valid attention at the first position\n",
    "}\n",
    "\n",
    "batch2 = {\n",
    "    'input_ids': torch.tensor([[2, 3]]),       # Two tokens\n",
    "    'attention_mask': torch.tensor([[1, 1]])  # Valid attention at two positions\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2\n"
     ]
    }
   ],
   "source": [
    "# Step #1 Determine Maximum Sequence Length\n",
    "\n",
    "seq1 = batch1['attention_mask'].shape[1]\n",
    "seq2 = batch2['attention_mask'].shape[1]\n",
    "\n",
    "print(seq1,seq2)\n",
    "max_seq_len = max(seq1,seq2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged Input IDs:\n",
      "tensor([[1, 0],\n",
      "        [2, 3]])\n",
      "Merged Attention Mask:\n",
      "tensor([[1, 0],\n",
      "        [1, 1]])\n"
     ]
    }
   ],
   "source": [
    "# Step2: Padding Sequences left\n",
    "\n",
    "max_seq_len = max(batch1['input_ids'].shape[1], batch2['input_ids'].shape[1])\n",
    "\n",
    "# Padding function to ensure both input_ids tensors have the same sequence length\n",
    "def pad_to_max_len(tensor, max_len):\n",
    "    padding_length = max_len - tensor.shape[1]\n",
    "    # Pad on the right to keep data intact on the left\n",
    "    return F.pad(tensor, (padding_length,0), 'constant', 0)\n",
    "\n",
    "# Pad 'input_ids' and 'attention_mask' for both batches and write back to the original dictionaries\n",
    "batch1['input_ids'] = pad_to_max_len(batch1['input_ids'], max_seq_len)\n",
    "batch1['attention_mask'] = pad_to_max_len(batch1['attention_mask'], max_seq_len)\n",
    "batch2['input_ids'] = pad_to_max_len(batch2['input_ids'], max_seq_len)\n",
    "batch2['attention_mask'] = pad_to_max_len(batch2['attention_mask'], max_seq_len)\n",
    "\n",
    "# Concatenating padded 'input_ids' and 'attention_mask'\n",
    "merged_input_ids = torch.cat([batch1['input_ids'], batch2['input_ids']], dim=0)\n",
    "merged_attention_mask = torch.cat([batch1['attention_mask'], batch2['attention_mask']], dim=0)\n",
    "\n",
    "print(\"Merged Input IDs:\")\n",
    "print(merged_input_ids)\n",
    "print(\"Merged Attention Mask:\")\n",
    "print(merged_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
